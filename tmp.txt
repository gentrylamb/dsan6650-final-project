# solvers/actor_critic.py
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from .base_solver import BaseSolver
from collections import deque
import random

# ---------------------------------------------------------
# Replay buffer for off-policy updates
# ---------------------------------------------------------
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        s, a, r, ns, d = zip(*batch)
        return np.array(s), np.array(a), np.array(r), np.array(ns), np.array(d)

    def __len__(self):
        return len(self.buffer)

# ---------------------------------------------------------
# Actor network (policy)
# ---------------------------------------------------------
class Actor(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_size=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
        )
        self.mean = nn.Linear(hidden_size, act_dim)
        self.log_std = nn.Linear(hidden_size, act_dim)

    def forward(self, x):
        h = self.net(x)
        mean = self.mean(h)
        log_std = self.log_std(h).clamp(-20, 2)
        std = log_std.exp()
        return mean, std

    def sample(self, x):
        mean, std = self(x)
        dist = torch.distributions.Normal(mean, std)
        action = dist.rsample()
        log_prob = dist.log_prob(action).sum(axis=-1)
        action = torch.tanh(action)  # bound actions between -1 and 1
        return action, log_prob

# ---------------------------------------------------------
# Critic network (Q-function)
# ---------------------------------------------------------
class Critic(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_size=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim + act_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        return self.net(x)

# ---------------------------------------------------------
# Off-policy SAC solver
# ---------------------------------------------------------
class ActorCriticSolver(BaseSolver):
    """
    Soft Actor-Critic (off-policy) for Appalachian Trail.
    Fully off-policy version compatible with the previous interface.
    """
    def __init__(self, env, lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2,
                 hidden_size=128, batch_size=64, buffer_size=10000, seed=None):
        super().__init__(env, seed)
        self.obs_dim = env.observation_space.shape[0]
        self.act_dim = env.action_space.n  # discrete actions (convert to one-hot)
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha
        self.batch_size = batch_size

        self.actor = Actor(self.obs_dim, self.act_dim, hidden_size)
        self.critic1 = Critic(self.obs_dim, self.act_dim, hidden_size)
        self.critic2 = Critic(self.obs_dim, self.act_dim, hidden_size)
        self.critic1_target = Critic(self.obs_dim, self.act_dim, hidden_size)
        self.critic2_target = Critic(self.obs_dim, self.act_dim, hidden_size)
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())

        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_opt = optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_opt = optim.Adam(self.critic2.parameters(), lr=lr)

        self.buffer = ReplayBuffer(buffer_size)

    # -----------------------------------------------------
    def act(self, state):
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        action_probs, _ = self.actor.sample(state_tensor)
        action_idx = int(torch.argmax(action_probs).item())
        return action_idx

    # -----------------------------------------------------
    def _update(self):
        if len(self.buffer) < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)
        states = torch.tensor(states, dtype=torch.float32)
        actions_onehot = torch.nn.functional.one_hot(torch.tensor(actions), num_classes=self.act_dim).float()
        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)
        next_states = torch.tensor(next_states, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)

        # -----------------------
        # Update Critic networks
        # -----------------------
        with torch.no_grad():
            next_actions, logp_next = self.actor.sample(next_states)
            q1_target = self.critic1_target(next_states, next_actions)
            q2_target = self.critic2_target(next_states, next_actions)
            q_target = torch.min(q1_target, q2_target) - self.alpha * logp_next.unsqueeze(-1)
            y = rewards + self.gamma * (1 - dones) * q_target

        q1 = self.critic1(states, actions_onehot)
        q2 = self.critic2(states, actions_onehot)
        critic1_loss = nn.MSELoss()(q1, y)
        critic2_loss = nn.MSELoss()(q2, y)

        self.critic1_opt.zero_grad()
        critic1_loss.backward()
        self.critic1_opt.step()

        self.critic2_opt.zero_grad()
        critic2_loss.backward()
        self.critic2_opt.step()

        # -----------------------
        # Update Actor network
        # -----------------------
        new_actions, logp = self.actor.sample(states)
        q1_pi = self.critic1(states, new_actions)
        q2_pi = self.critic2(states, new_actions)
        q_pi = torch.min(q1_pi, q2_pi)
        actor_loss = (self.alpha * logp.unsqueeze(-1) - q_pi).mean()

        self.actor_opt.zero_grad()
        actor_loss.backward()
        self.actor_opt.step()

        # -----------------------
        # Soft update targets
        # -----------------------
        for target_param, param in zip(self.critic1_target.parameters(), self.critic1.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        for target_param, param in zip(self.critic2_target.parameters(), self.critic2.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    # -----------------------------------------------------
    def train(self, episodes=500):
        episode_rewards = []
        failure_reasons = []

        for ep in range(episodes):
            state, info = self.env.reset()
            done = False
            ep_reward = 0

            while not done:
                action = self.act(state)
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated

                # Store in replay buffer
                self.buffer.push(state, action, reward, next_state, done)

                ep_reward += reward
                state = next_state

                if done:
                    failure_reasons.append(info.get("failure_reason", "Unknown"))

                # Perform SAC updates at every step
                self._update()

            episode_rewards.append(ep_reward)
            if (ep + 1) % 50 == 0:
                print(f"Episode {ep+1}/{episodes} | Reward: {ep_reward:.1f}")

        return episode_rewards, failure_reasons


# solvers/base_solver.py
from abc import ABC, abstractmethod
import numpy as np

class BaseSolver(ABC):
    """
    Abstract base class for all RL solvers.

    All solvers must implement:
    - act(state): choose an action
    - train(episodes): run training loop

    This class provides:
    - env reference
    - RNG
    - run_episode() helper for consistency
    """
    def __init__(self, env, seed=None):
        self.env = env
        self.rng = np.random.default_rng(seed)
    
     # -----------------------------------------------------
    @abstractmethod
    def act(self, state):
        """Return an action given the current state."""
        pass

    # -----------------------------------------------------
    @abstractmethod
    def train(self, episodes: int):
        """Train the agent for a number of episodes."""
        pass
    
    # -----------------------------------------------------
    def run_episode(self, render=False):
        """
        Utility helper: run a single episode using self.act().
        Useful for evaluation or debugging.
        """

        state, _ = self.env.reset()
        total_reward = 0
        done = False

        while not done:
            action = self.act(state)
            next_state, reward, terminated, truncated, info = self.env.step(action)

            total_reward += reward
            state = next_state

            done = terminated or truncated

            if render:
                self.env.render()

        return total_reward
# solvers/policy_gradient.py

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

from .base_solver import BaseSolver


class PolicyNetwork(nn.Module):
    """
    Simple MLP policy π(a|s) producing action probabilities.
    """
    def __init__(self, input_dim, output_dim, hidden_size=128):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.model(x)


class PolicyGradientSolver(BaseSolver):
    """
    Monte Carlo Policy Gradient implementation.
    """

    def __init__(
        self,
        env,
        lr=1e-3,
        gamma=0.99,
        hidden_size=128,
        reward_scale=50.0,
        seed=None
    ):
        super().__init__(env, seed)

        obs_dim = env.observation_space.shape[0]
        act_dim = env.action_space.n

        self.gamma = gamma
        self.reward_scale = reward_scale

        # Create policy network
        self.policy = PolicyNetwork(obs_dim, act_dim, hidden_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # Store trajectories
        self.log_probs = []
        self.rewards = []

    # -----------------------------------------------------
    def act(self, state):
        """
        Sample an action from π(a|s).
        """

        state_tensor = torch.tensor(state, dtype=torch.float32)
        probs = self.policy(state_tensor)

        # Categorical sampling
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()

        # store log-prob for training
        log_prob = dist.log_prob(action)
        self.log_probs.append(log_prob)

        return int(action.item())

    # -----------------------------------------------------
    def _compute_returns(self):
        """
        Compute discounted returns G_t for each step:
        G_t = r_t + gamam*r_{t+1} + gamma^2*r_{t+2} + ...
        """
        returns = []
        G = 0

        for r in reversed(self.rewards):
            G = r + self.gamma * G
            returns.append(G)

        returns.reverse()
        return returns

    # -----------------------------------------------------
    def _update_policy(self):
        """
        Apply REINFORCE gradient update:
        ∇J = E[ G_t ∇ log π(a_t|s_t) ]
        """
        returns = self._compute_returns()

        # Normalize returns for training stability
        returns = torch.tensor(returns, dtype=torch.float32)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        loss = 0
        for log_prob, Gt in zip(self.log_probs, returns):
            loss -= log_prob * Gt   # gradient ascent via negative loss

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Clear buffers
        self.log_probs.clear()
        self.rewards.clear()

    # -----------------------------------------------------
    def train(self, episodes=500):
        """
        Train using full-episode rollouts with REINFORCE.
        """

        episode_rewards = []
        failure_reasons = []

        for ep in range(episodes):
            state, _ = self.env.reset()
            done = False
            ep_reward = 0

            while not done:
                action = self.act(state)
                next_state, reward, terminated, truncated, info = self.env.step(action)

                # Normalize reward
                norm_reward = reward / self.reward_scale

                self.rewards.append(norm_reward)
                ep_reward += reward

                state = next_state
                done = terminated or truncated

                # if episode ended, record reason
                if done:
                    failure_reasons.append(info.get('failure_reason', 'Unknown'))

            # Episode finished → update policy
            self._update_policy()

            episode_rewards.append(ep_reward)

            if (ep + 1) % 50 == 0:
                print(f"Episode {ep+1}/{episodes} | Reward: {ep_reward:.1f}")

        return episode_rewards, failure_reasons# solvers/q_learning.py
import numpy as np
from .base_solver import BaseSolver


class QLearningSolver(BaseSolver):
    """
    Tabular Q-learning agent for the Appalachian Trail environment.
    Continuous state variables are discretized into bins.
    """

    def __init__(
        self,
        env,
        learning_rate=0.1,
        gamma=0.99,
        epsilon=0.1,
        bins=(40, 20, 20, 3, 200), 
        # bins=(10, 5, 3, 10),        # for env without energy
        epsilon_decay=None,         # optional
        seed=None
    ):
        super().__init__(env, seed)

        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay

        self.bins = bins
        self.bin_edges = self._create_bin_edges()

        # Q-table shape: (#bins_0, ..., #bins_4, #actions)
        self.q_table = np.zeros((*bins, env.action_space.n))

    # -----------------------------------------------------
    # State Discretization
    # -----------------------------------------------------
    def _create_bin_edges(self):
        lows = self.env.observation_space.low
        highs = self.env.observation_space.high

        edges = []
        for low, high, num in zip(lows, highs, self.bins):
            # Ensure proper scaling for large ranges
            edges.append(np.linspace(low, high, num + 1)[1:-1])
        return edges

    def _discretize(self, state):
        # Clip state to avoid OOB values
        low = self.env.observation_space.low
        high = self.env.observation_space.high
        state = np.clip(state, low, high)

        return tuple(
            np.digitize(s, edges)
            for s, edges in zip(state, self.bin_edges)
        )

    # -----------------------------------------------------
    # Action Selection (ε-greedy)
    # -----------------------------------------------------
    def act(self, state):
        if self.rng.random() < self.epsilon:
            return self.env.action_space.sample()

        s = self._discretize(state)
        return int(np.argmax(self.q_table[s]))

    # -----------------------------------------------------
    # Training
    # -----------------------------------------------------
    def train(self, episodes=5000):
        rewards = []
        failure_reasons = []

        for ep in range(episodes):
            state, _ = self.env.reset()
            done = False
            total_r = 0

            while not done:
                s_disc = self._discretize(state)

                # ε-greedy action
                if self.rng.random() < self.epsilon:
                    action = self.env.action_space.sample()
                else:
                    action = int(np.argmax(self.q_table[s_disc]))

                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                s_next_disc = self._discretize(next_state)

                # Normalize rewards mildly to stabilize Q updates
                norm_reward = reward / 50.0

                # Terminal state handling
                if done:
                    td_target = norm_reward
                    failure_reasons.append(info.get("failure_reason", "Unknown"))
                else:
                    td_target = norm_reward + self.gamma * np.max(self.q_table[s_next_disc])

                td_error = td_target - self.q_table[s_disc][action]
                self.q_table[s_disc][action] += self.lr * td_error

                state = next_state
                total_r += reward

            # Optional epsilon decay
            if self.epsilon_decay is not None:
                self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)

            rewards.append(total_r)

            if (ep + 1) % 50 == 0:
                print(f"Episode {ep+1}/{episodes} | Reward: {total_r:.1f}")

        return rewards, failure_reasons
# solvers/random.py
import numpy as np


class RandomAgent():
    """
    Agent that makes random acts as a baseline to compare.
    """
    def __init__(self, env, seed=None):
        self.env = env
        self.rng = np.random.default_rng(seed)

    def act():
        pass

    def train(self, episodes=1000):
        rewards = []
        failure_reasons = []

        for ep in range(episodes):
            state, _ = self.env.reset()
            done = False
            total_r = 0
            while not done:
                action = self.env.action_space.sample()
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                total_r += reward
                state = next_state

                # if episode ended, record reason
                if done:
                    failure_reasons.append(info.get('failure_reason', 'Unknown'))
            
            rewards.append(total_r)
            
            if (ep + 1) % 50 == 0:
                print(f"Episode {ep+1}/{episodes} | Reward: {total_r:.1f}")

        return rewards, failure_reasons# solvers/sarsa.py
import numpy as np
from .base_solver import BaseSolver


class SarsaSolver(BaseSolver):
    """
    Tabular SARSA (on-policy TD) solver for the Appalachian Trail environment.
    Continuous state variables are discretized into bins.
    """

    def __init__(
        self,
        env,
        learning_rate=0.1,
        gamma=0.99,
        epsilon=0.1,
        bins=(40, 20, 20, 3, 200),
        # bins=(10, 5, 3, 10),         # for env without energy
        epsilon_decay=None,          # optional
        seed=None
    ):
        super().__init__(env, seed)

        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay

        # Discretization setup
        self.bins = bins
        self.bin_edges = self._create_bin_edges()

        # Q-table
        self.q_table = np.zeros((*bins, env.action_space.n))

    # -----------------------------------------------------
    # Discretization helpers
    # -----------------------------------------------------
    def _create_bin_edges(self):
        lows = self.env.observation_space.low
        highs = self.env.observation_space.high

        edges = []
        for low, high, num in zip(lows, highs, self.bins):
            edges.append(np.linspace(low, high, num + 1)[1:-1])
        return edges

    def _discretize(self, state):
        # Clip state to avoid going outside bin edges
        low = self.env.observation_space.low
        high = self.env.observation_space.high
        state = np.clip(state, low, high)

        return tuple(
            np.digitize(s, edges)
            for s, edges in zip(state, self.bin_edges)
        )

    # -----------------------------------------------------
    def act(self, state):
        """ε-greedy action selection."""
        if self.rng.random() < self.epsilon:
            return self.env.action_space.sample()
        return int(np.argmax(self.q_table[self._discretize(state)]))

    # -----------------------------------------------------
    def train(self, episodes=5000):
        rewards = []
        failure_reasons = []

        for ep in range(episodes):
            state, info = self.env.reset()
            s_disc = self._discretize(state)

            # Choose initial action
            if self.rng.random() < self.epsilon:
                action = self.env.action_space.sample()
            else:
                action = int(np.argmax(self.q_table[s_disc]))

            total_r = 0
            done = False

            while not done:
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                total_r += reward

                s_next_disc = self._discretize(next_state)

                # Normalize reward to stabilize learning
                norm_reward = reward / 50.0

                # Choose next action (on-policy)
                if not done:
                    if self.rng.random() < self.epsilon:
                        next_action = self.env.action_space.sample()
                    else:
                        next_action = int(np.argmax(self.q_table[s_next_disc]))
                else:
                    next_action = None  # terminal

                # SARSA target
                if done:
                    td_target = norm_reward
                    failure_reasons.append(info.get("failure_reason", "Unknown"))
                else:
                    td_target = norm_reward + self.gamma * self.q_table[s_next_disc][next_action]

                td_error = td_target - self.q_table[s_disc][action]

                # Update
                self.q_table[s_disc][action] += self.lr * td_error

                # Advance
                state = next_state
                s_disc = s_next_disc
                action = next_action

            # Optional epsilon decay
            if self.epsilon_decay is not None:
                self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)

            rewards.append(total_r)

            if (ep + 1) % 50 == 0:
                print(f"Episode {ep+1}/{episodes} | Reward: {total_r:.1f}")

        return rewards, failure_reasons
        
# envs/trail.py
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd

class AppalachianTrailEnv(gym.Env):
    """
    A simplified reinforcement learning environment simulating a thru-hike 
    of the Appalachian Trail. The agent must manage energy, food, and time 
    while deciding how far to hike, when to rest, and when to resupply.
    """

    metadata = {"render_modes": ["human"]}

    def __init__(self,
                 trail_length=2200,    # Total distance (2,197.4 miles)
                 max_energy=100,       # Energy (percent of 100)
                 max_food=10,          # Number of days of food
                 max_days = 213,       # Approx. 7 months
                 seed=None):

        super().__init__()
        self.trail_length = trail_length
        self.max_energy = max_energy
        self.max_food = max_food
        self.max_days = max_days         
        self.rng = np.random.default_rng(seed)

        # Resupply points (https://whiteblaze.net/forum/content.php/1344-Resuppling-within-one-miles-from-the-Appalachian-Trail-for-a-thru-hike)
        self.resupply_points = [round(item) for item in pd.read_csv("data/resupply_points.csv")['mile'].tolist()]

        # --- Observation Space ---
        # [miles_remaining, energy, food, weather, day]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0], dtype=np.float32),
            high=np.array([trail_length, max_energy, max_food, 2, max_days], dtype=np.float32),
            dtype=np.float32
        )

        # --- Action Space ---
        # 0 = hike easy day (8-12 miles)
        # 1 = hike standard day (13-18 miles)
        # 2 = hike big day (19-25 miles)
        # 3 = rest/zero day (0 miles)
        # 4 = resupply day (only valid at stops)
        self.action_space = spaces.Discrete(5)

        self.reset()

    # --------------------------------------------------------

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)

        self.miles_remaining = float(self.trail_length)
        self.energy = float(self.max_energy)
        self.food = float(self.max_food)
        self.day = 0

        # 0 = clear, 1 = rain, 2 = storm
        self.weather = 0  

        return self._get_obs(), {}

    # --------------------------------------------------------

    def _get_obs(self):
        return np.array([
            self.miles_remaining,
            self.energy,
            self.food,
            self.weather,
            self.day
        ], dtype=np.float32)
    
    # --------------------------------------------------------

    def _calculate_distance_to_resupply(self):
        # get current mile and find distance to closest resupply
        current_mile = self.trail_length - self.miles_remaining
        next_resupply = min([p for p in self.resupply_points if p > current_mile], default=float('inf'))

        return next_resupply - current_mile
    
    # --------------------------------------------------------

    def step(self, action):
        done = False
        info = {}
        reward = 0

        # Daily weather update
        self.weather = self.rng.choice([0, 1, 2], p=[0.7, 0.2, 0.1])  # mostly clear

        # ----------------------------------------------------
        # Action Effects
        # ----------------------------------------------------

        if action == 0:   # Hike easy day (8-12 miles)
            miles = np.random.randint(8,13)
            energy_cost = 0.5*miles
            food_cost = 0.8 

        elif action == 1: # Hike standard day (13-18 miles)
            miles = np.random.randint(13,19)
            energy_cost = 0.75*miles
            food_cost = 1

        elif action == 2: # Hike big day (19-25 miles)
            miles = np.random.randint(18,26)
            energy_cost = 1*miles
            food_cost = 1.1

        elif action == 3: # Rest/zero day (0 miles)
            miles = 0
            energy_cost = -80   # 80% recovery
            food_cost = 0.5

        elif action == 4: # Resupply day
            dist = self._calculate_distance_to_resupply()
            if dist > 15:
                # invalid action
                print("   INVALID ACTION: No resupply point nearby.")
                miles = 0
                energy_cost = 0
                food_cost = 0
                # penalty
                reward -= 10

            else:
                # valid action
                miles = dist
                energy_cost = -50
                food_cost = 0
                # refill and reward
                self.food = self.max_food  # full refill
                reward += 10

        # Weather modifies energy cost (only for expenditure)
        if energy_cost > 0:
            if self.weather == 1: energy_cost *= 1.1  # rain
            elif self.weather == 2: energy_cost *= 1.3 # storm
            
        food_cost *= 0.2
        energy_cost *= 0.2
        # Apply transitions
        self.miles_remaining = max(0, self.miles_remaining - miles)
        self.energy = np.clip(self.energy - energy_cost, 0, self.max_energy)
        self.food = max(0, self.food - food_cost)
        self.day += 1

        # ----------------------------------------------------
        # Reward Structure
        # ----------------------------------------------------
        # Goal: minimize days, avoid running out of food/energy

        # time penalty
        reward -= 1 
        # progress reward
        if miles > 0: reward += miles * 1
        # penalties for low reserves
        if self.energy <= 5: reward -= 25  # collapse risk
        if self.food <= .5: reward -= 25  # starvation risk

        # Completion Case
        if self.miles_remaining <= 0:
            print("   SUCCESS: Congrats, you completed the AT!")
            info["failure_reason"] = "Success"
            reward += 300
            done = True

        # Failure Cases
        if self.energy <= 0:
            print("   FAILURE: You ran out of energy!")
            info["failure_reason"] = "Energy"
            reward -= 100
            done = True

        if self.food <= 0:
            print("   FAILURE: You ran out of food!")
            info["failure_reason"] = "Food"
            reward -= 100
            done = True
            
        if self.day >= self.max_days:
            print("   FAILURE: You took too long!")
            info["failure_reason"] = "Time"
            reward -= 100
            done = True
        
        return self._get_obs(), reward, done, False, info

    # --------------------------------------------------------

    def render(self):
        print(f"Day {self.day}: {self.miles_remaining:.1f} miles left | "
              f"Energy {self.energy:.1f} | Food {self.food:.1f} | Weather {self.weather}")
